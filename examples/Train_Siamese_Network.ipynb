{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train Siamese Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kaliostrogoblin/PDD/blob/master/examples/Train_Siamese_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4F2DbOsuRJ3t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Siamese Network for PDD data\n",
        "\n",
        "In this example we will show, how to train your own classifier using [Plant Disease Database](http://pdd.jinr.ru/db/)"
      ]
    },
    {
      "metadata": {
        "id": "lEQsOtGHSBmO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Cloning the repo\n",
        "\n",
        "At first we will clone the repository."
      ]
    },
    {
      "metadata": {
        "id": "VKEiGYnzc5fH",
        "colab_type": "code",
        "outputId": "f3dc4d9d-ac07-4798-a7f6-b36bf238fb3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -r -f PDD\n",
        "!git clone https://github.com/Kaliostrogoblin/PDD.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PDD'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 227 (delta 0), reused 2 (delta 0), pack-reused 222\u001b[K\n",
            "Receiving objects: 100% (227/227), 53.90 MiB | 24.59 MiB/s, done.\n",
            "Resolving deltas: 100% (124/124), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "98kZWpCZST5Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Change the directory to PDD to get access of helper functions and classes."
      ]
    },
    {
      "metadata": {
        "id": "qko1eieXbXta",
        "colab_type": "code",
        "outputId": "f54e3c27-40a3-4f6c-8038-01387a607391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('PDD')\n",
        "# verify if we are in correct directory\n",
        "os.listdir()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.gitignore', 'server', 'examples', 'pdd', 'README.md', '.git']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "2JZzbdilTFMp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Downloading the grape pdd dataset \n",
        "\n",
        "We load it and split on train and test subdirectories in place."
      ]
    },
    {
      "metadata": {
        "id": "mzeAAMVebk27",
        "colab_type": "code",
        "outputId": "3c8ca144-80d0-4484-f053-8dc3ad070567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "cell_type": "code",
      "source": [
        "from pdd.datasets.grape import load_data\n",
        "\n",
        "train_data_path, test_data_path = load_data(split_on_train_test=True, random_state=13)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://pdd.jinr.ru/images/base/grape/grape.tar\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "34845it [00:10, 3316.43it/s]                             \n",
            "100%|████████████████████| 10/10 [00:00<00:00, 75.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Splitting on train and test...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "PYGPbc_DVxYS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The structure of the dataset catalogue now:\n",
        "\n",
        "```\n",
        "grape\n",
        "│    \n",
        "└───train\n",
        "│   └───black_rot\n",
        "│   |   │   20.jpg\n",
        "│   |   │   ...\n",
        "|   |\n",
        "|   └───chlorosis\n",
        "|   └───esca\n",
        "|   └───healty\n",
        "|   └───mildew\n",
        "|\n",
        "└───test\n",
        "    └───black_rot\n",
        "    |   │   3.jpg\n",
        "    |   │   ...\n",
        "    |\n",
        "    └───chlorosis\n",
        "    └───esca\n",
        "    └───healty\n",
        "    └───mildew\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "2SnU7VEaVzlD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a feature extractor `twin` and a siamese network"
      ]
    },
    {
      "metadata": {
        "id": "Gg4HAly4duNq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pdd.models import get_feature_extractor\n",
        "from pdd.models import make_siamese\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "# set the single session for tensorflow and keras both\n",
        "sess = tf.Session()\n",
        "K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RBkgrrT1W_Gh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are using the cross-entropy loss as in [this paper](http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) instead of contrastive loss. \n",
        "\n",
        "But feel free for using it by changing parameter `loss` to 'contrastive'. E.g.\n",
        "\n",
        "```python\n",
        "siams = make_siamese(feature_extractor, dist='l2', loss='contrastive')\n",
        "```\n",
        "\n",
        "There are three types of distances:\n",
        "\n",
        "\n",
        "*   `'l1'`\n",
        "*   `'l2'`\n",
        "*   `'cosine'`\n",
        "\n",
        "**But only `'l1'` is available for cross-entropy loss.**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1dlZXUPReNMA",
        "colab_type": "code",
        "outputId": "11cac8be-0b8d-4c2d-fcaa-d491905b883d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "cell_type": "code",
      "source": [
        "input_shape = (256, 256, 3)\n",
        "\n",
        "print(\"Building feature extractor...\")\n",
        "feature_extractor = get_feature_extractor(input_shape)\n",
        "\n",
        "print(\"Constructing siamese network...\")\n",
        "siams = make_siamese(feature_extractor, dist='l1', loss='cross_entropy')\n",
        "siams.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building feature extractor...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Constructing siamese network...\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Model)                 (None, 1024)         14902496    input_2[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 1024)         0           model_1[1][0]                    \n",
            "                                                                 model_1[2][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            1025        lambda_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 14,903,521\n",
            "Trainable params: 14,901,537\n",
            "Non-trainable params: 1,984\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2djD033wYVAe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating a batch generator\n",
        "\n",
        "To train a siamese network data should be passed to the input as **positive-negative pairs**. Positive pair of images means a pair consists of images from the same class. Negative pairs consist of images of different classes."
      ]
    },
    {
      "metadata": {
        "id": "tjJ1yf3ppqgs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pdd.utils.training import SiameseBatchGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a_iYx0D9ZNlu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For training we are using a strong augmentation including rotations, zooming, flips and channel shifts."
      ]
    },
    {
      "metadata": {
        "id": "4sxDA67Ep01t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_batch_gen = SiameseBatchGenerator.from_directory(dirname=train_data_path, augment=True)\n",
        "test_batch_gen = SiameseBatchGenerator.from_directory(dirname=test_data_path)\n",
        "\n",
        "def siams_generator(batch_gen, batch_size=None):\n",
        "    while True:\n",
        "        batch_xs, batch_ys = batch_gen.next_batch(batch_size)\n",
        "        yield [batch_xs[0], batch_xs[1]], batch_ys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QJW0ECMjZf67",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "Let's train the model. \n",
        "\n",
        "**Note**, despite the fact that we are training the ыiamese model, the feature extractor is also being trained."
      ]
    },
    {
      "metadata": {
        "id": "d-2WjRv1eK9E",
        "colab_type": "code",
        "outputId": "0497186a-d61a-46f6-cbc7-2aadb6bfe832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1714
        }
      },
      "cell_type": "code",
      "source": [
        "siams.fit_generator(\n",
        "    generator=siams_generator(train_batch_gen),\n",
        "    steps_per_epoch=100,\n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    validation_data=siams_generator(test_batch_gen),\n",
        "    validation_steps=30,\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "100/100 [==============================] - 165s 2s/step - loss: 0.7295 - acc: 0.5353 - val_loss: 0.6761 - val_acc: 0.5656\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 158s 2s/step - loss: 0.7211 - acc: 0.5572 - val_loss: 0.6701 - val_acc: 0.5979\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.7106 - acc: 0.5672 - val_loss: 0.7013 - val_acc: 0.5781\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 158s 2s/step - loss: 0.7099 - acc: 0.5634 - val_loss: 0.6320 - val_acc: 0.6385\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.6636 - acc: 0.6097 - val_loss: 0.6729 - val_acc: 0.6188\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 158s 2s/step - loss: 0.6703 - acc: 0.6059 - val_loss: 0.5866 - val_acc: 0.6854\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.6380 - acc: 0.6369 - val_loss: 0.6394 - val_acc: 0.6646\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 158s 2s/step - loss: 0.6131 - acc: 0.6481 - val_loss: 0.5774 - val_acc: 0.6698\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 158s 2s/step - loss: 0.6014 - acc: 0.6688 - val_loss: 0.5621 - val_acc: 0.7073\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.5923 - acc: 0.6709 - val_loss: 0.5145 - val_acc: 0.7427\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.5770 - acc: 0.6887 - val_loss: 0.6212 - val_acc: 0.6750\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.5348 - acc: 0.7297 - val_loss: 0.6156 - val_acc: 0.6917\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.5382 - acc: 0.7153 - val_loss: 0.6206 - val_acc: 0.6562\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.4999 - acc: 0.7525 - val_loss: 0.5425 - val_acc: 0.7344\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.5048 - acc: 0.7403 - val_loss: 0.5398 - val_acc: 0.7375\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.4794 - acc: 0.7613 - val_loss: 0.5004 - val_acc: 0.7615\n",
            "Epoch 17/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.4945 - acc: 0.7594 - val_loss: 0.5388 - val_acc: 0.7844\n",
            "Epoch 18/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.4623 - acc: 0.7781 - val_loss: 0.5598 - val_acc: 0.7260\n",
            "Epoch 19/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.4135 - acc: 0.8088 - val_loss: 0.5873 - val_acc: 0.7438\n",
            "Epoch 20/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.4115 - acc: 0.8072 - val_loss: 0.5399 - val_acc: 0.6969\n",
            "Epoch 21/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.3904 - acc: 0.8159 - val_loss: 0.4369 - val_acc: 0.8187\n",
            "Epoch 22/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.3750 - acc: 0.8253 - val_loss: 0.5344 - val_acc: 0.8021\n",
            "Epoch 23/50\n",
            "100/100 [==============================] - 155s 2s/step - loss: 0.3652 - acc: 0.8387 - val_loss: 0.3804 - val_acc: 0.8115\n",
            "Epoch 24/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.3416 - acc: 0.8497 - val_loss: 0.3596 - val_acc: 0.8510\n",
            "Epoch 25/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.3336 - acc: 0.8516 - val_loss: 0.5923 - val_acc: 0.7240\n",
            "Epoch 26/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.3052 - acc: 0.8697 - val_loss: 0.3112 - val_acc: 0.8427\n",
            "Epoch 27/50\n",
            "100/100 [==============================] - 158s 2s/step - loss: 0.2862 - acc: 0.8713 - val_loss: 0.4919 - val_acc: 0.7875\n",
            "Epoch 28/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.2840 - acc: 0.8800 - val_loss: 0.4094 - val_acc: 0.8427\n",
            "Epoch 29/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.2645 - acc: 0.8916 - val_loss: 0.3321 - val_acc: 0.8510\n",
            "Epoch 30/50\n",
            "100/100 [==============================] - 159s 2s/step - loss: 0.2610 - acc: 0.8912 - val_loss: 0.3081 - val_acc: 0.8698\n",
            "Epoch 31/50\n",
            "100/100 [==============================] - 158s 2s/step - loss: 0.2350 - acc: 0.8981 - val_loss: 0.3946 - val_acc: 0.8375\n",
            "Epoch 32/50\n",
            "100/100 [==============================] - 158s 2s/step - loss: 0.2155 - acc: 0.9166 - val_loss: 0.3714 - val_acc: 0.8167\n",
            "Epoch 33/50\n",
            "100/100 [==============================] - 158s 2s/step - loss: 0.2212 - acc: 0.9122 - val_loss: 0.3385 - val_acc: 0.8708\n",
            "Epoch 34/50\n",
            "100/100 [==============================] - 159s 2s/step - loss: 0.2209 - acc: 0.9075 - val_loss: 0.5151 - val_acc: 0.7781\n",
            "Epoch 35/50\n",
            "100/100 [==============================] - 158s 2s/step - loss: 0.1940 - acc: 0.9237 - val_loss: 0.2714 - val_acc: 0.8615\n",
            "Epoch 36/50\n",
            "100/100 [==============================] - 158s 2s/step - loss: 0.1662 - acc: 0.9394 - val_loss: 0.3158 - val_acc: 0.8708\n",
            "Epoch 37/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.1685 - acc: 0.9391 - val_loss: 0.2490 - val_acc: 0.9448\n",
            "Epoch 38/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.1926 - acc: 0.9225 - val_loss: 0.3724 - val_acc: 0.8521\n",
            "Epoch 39/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.1608 - acc: 0.9381 - val_loss: 0.3310 - val_acc: 0.8635\n",
            "Epoch 40/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.1674 - acc: 0.9350 - val_loss: 0.3032 - val_acc: 0.8698\n",
            "Epoch 41/50\n",
            "100/100 [==============================] - 158s 2s/step - loss: 0.1386 - acc: 0.9537 - val_loss: 0.3552 - val_acc: 0.8646\n",
            "Epoch 42/50\n",
            "100/100 [==============================] - 159s 2s/step - loss: 0.1450 - acc: 0.9434 - val_loss: 0.2154 - val_acc: 0.9042\n",
            "Epoch 43/50\n",
            "100/100 [==============================] - 158s 2s/step - loss: 0.1309 - acc: 0.9519 - val_loss: 0.4797 - val_acc: 0.7875\n",
            "Epoch 44/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.1359 - acc: 0.9513 - val_loss: 0.3546 - val_acc: 0.8792\n",
            "Epoch 45/50\n",
            "100/100 [==============================] - 158s 2s/step - loss: 0.1153 - acc: 0.9587 - val_loss: 0.4303 - val_acc: 0.8302\n",
            "Epoch 46/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.1125 - acc: 0.9594 - val_loss: 0.3924 - val_acc: 0.8208\n",
            "Epoch 47/50\n",
            "100/100 [==============================] - 157s 2s/step - loss: 0.1157 - acc: 0.9569 - val_loss: 0.5946 - val_acc: 0.8135\n",
            "Epoch 48/50\n",
            "100/100 [==============================] - 154s 2s/step - loss: 0.1092 - acc: 0.9625 - val_loss: 0.3471 - val_acc: 0.8667\n",
            "Epoch 49/50\n",
            "100/100 [==============================] - 155s 2s/step - loss: 0.1006 - acc: 0.9634 - val_loss: 0.2266 - val_acc: 0.8979\n",
            "Epoch 50/50\n",
            "100/100 [==============================] - 156s 2s/step - loss: 0.1113 - acc: 0.9631 - val_loss: 0.4187 - val_acc: 0.8479\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f26eb90c0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "OGUMr-KnamTW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### K-nearest neighbors for classification\n",
        "\n",
        "Using of a siamese network for classification requires to perform an iterative n-shot task. But to avoid this restriction we can build a KNN classifier with the help of features extracted using the `twin` network.\n",
        "\n",
        "To significantly speed up the inference phase of the classifer we are going to utilize a fast k-nearest-neighbour search based on the method used in [\"Learning To Remember Rare Events\"](https://openreview.net/pdf?id=SJTQLdqlg)."
      ]
    },
    {
      "metadata": {
        "id": "LbCgMWIY4tG1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Save the feature extractor model and clear session"
      ]
    },
    {
      "metadata": {
        "id": "MDT1R5wq44PF",
        "colab_type": "code",
        "outputId": "64bfd424-2cac-45e9-88c2-b655aeb617ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Saving feature extractor...\")\n",
        "feature_extractor.save('pdd_feature_extractor.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving feature extractor...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VRFl_pk_4_EQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "tf.reset_default_graph()\n",
        "del sess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xxjxy5jW5Kj7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Load the feature exctrator to KNN model"
      ]
    },
    {
      "metadata": {
        "id": "AGpTRoY7mnoD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from pdd.models import TfKNN\n",
        "from pdd.utils.data_utils import create_dataset_from_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SBI1E8q05iEl",
        "colab_type": "code",
        "outputId": "e538f3d2-f85d-4c3c-8fac-8967074d77c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "sess = tf.Session()\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_session(sess)\n",
        "\n",
        "print(\"Loading feature extractor...\")\n",
        "feature_extractor = load_model(\"pdd_feature_extractor.h5\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading feature extractor...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "tDvsUnvd5lsB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Load datasets for the evaluation"
      ]
    },
    {
      "metadata": {
        "id": "58HF_Dtx5qhe",
        "colab_type": "code",
        "outputId": "1eb5de13-aa72-4f25-b381-0d596ff73c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Loading datasets...\")\n",
        "train_dataset = create_dataset_from_dir(train_data_path, shuffle=True)\n",
        "test_dataset = create_dataset_from_dir(test_data_path, shuffle=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading datasets...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cam8MkNP5vGJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Create KNN graph"
      ]
    },
    {
      "metadata": {
        "id": "_GwDgKQI5ysp",
        "colab_type": "code",
        "outputId": "e70b6334-2d43-4119-ef46-8780b2e11e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "cell_type": "code",
      "source": [
        "tfknn = TfKNN(sess, \n",
        "              feature_extractor, \n",
        "              (train_dataset['data'], train_dataset['target']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting keys from support set...\n",
            "Took 1.48 seconds to run.\n",
            "\n",
            "Freezing feature extractor graph...\n",
            "INFO:tensorflow:Froze 52 variables.\n",
            "INFO:tensorflow:Converted 52 variables to const ops.\n",
            "Took 0.39 seconds to run.\n",
            "\n",
            "Creating TfKNN graph...\n",
            "Took 0.02 seconds to run.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ts1wL9AB53De",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Evaluate"
      ]
    },
    {
      "metadata": {
        "id": "41rTBE78nnRa",
        "colab_type": "code",
        "outputId": "acd0091e-5477-4765-badf-cad5c9a4dfda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# predictions and similarities\n",
        "preds, sims = tfknn.predict(test_dataset['data'])\n",
        "accuracy = accuracy_score(test_dataset['target'], preds)\n",
        "print(\"Accuracy: %.2f\" % accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vmGLtcsd5_sO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Save the model for tensorflow serving"
      ]
    },
    {
      "metadata": {
        "id": "XTUZ9Kik6DT5",
        "colab_type": "code",
        "outputId": "b8f2537e-7ea4-4380-b743-13a9c5a29b88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "tfknn.save_graph_for_serving(\"tfknn_graph\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving graph for serving...\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: tfknn_graph/saved_model.pb\n",
            "Took 0.29 seconds to run.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pkTlwqZ26xSf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Save and upload model to Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "dBgInXZzgcTf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Mount your Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "Klc3pnQrgQLI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wxWxzVG6gq_I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copy files to gdrive"
      ]
    },
    {
      "metadata": {
        "id": "IirXa6t1gxyP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make directory in the destination\n",
        "!mkdir \"/gdrive/My Drive/pdd_model\"\n",
        "\n",
        "# copy models \n",
        "!cp tfknn_graph/saved_model.pb \"/gdrive/My Drive/pdd_model/tf_graph.pb\"\n",
        "!cp pdd_feature_extractor.h5 \"/gdrive/My Drive/pdd_model/pdd_feature_extractor.h5\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}